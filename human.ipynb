{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLOv8 pre-trained model\n",
    "model = YOLO('yolov8n.pt')  # Use 'yolov8n.pt' for a smaller model, or a custom model trained for human detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'deep_sort'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/nwojke/deep_sort.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'nn_matching' from 'deep_sort' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeep_sort\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeep_sort_app\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeepSort\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load YOLOv8 model\u001b[39;00m\n",
      "File \u001b[1;32md:\\AIDSDept\\SIH24\\day1 mentoring\\checkhttp\\deep_sort\\deep_sort_app.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapplication_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocessing\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mapplication_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m visualization\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeep_sort\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn_matching\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeep_sort\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdetection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Detection\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeep_sort\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtracker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tracker\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'nn_matching' from 'deep_sort' (unknown location)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from deep_sort.deep_sort_app import DeepSort\n",
    "import torch\n",
    "\n",
    "# Load YOLOv8 model\n",
    "yolo_model = YOLO('yolov8n.pt')  # YOLOv8 model\n",
    "\n",
    "# Initialize DeepSORT\n",
    "deepsort = DeepSort(model_path='ckpt.t7')  # Path to DeepSORT model weights\n",
    "\n",
    "# Load video\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # YOLOv8 detection\n",
    "    results = yolo_model(frame)\n",
    "    detections = results.xywh[0]  # Bounding boxes with xywh format\n",
    "\n",
    "    # Filter detections for humans (class 0 in COCO dataset is for 'person')\n",
    "    humans = [det for det in detections if det[5] == 0]\n",
    "\n",
    "    # Convert detections to the format required by DeepSORT\n",
    "    bbox_xywh = [[x1, y1, w, h] for x1, y1, w, h, conf, cls in humans]\n",
    "    confs = [conf for x1, y1, w, h, conf, cls in humans]\n",
    "\n",
    "    # Perform tracking with DeepSORT\n",
    "    outputs = deepsort.update(bbox_xywh, confs, frame)\n",
    "\n",
    "    # Draw results\n",
    "    for output in outputs:\n",
    "        x1, y1, x2, y2, track_id = output\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"ID: {track_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 (no detections), 114.7ms\n",
      "Speed: 3.0ms preprocess, 114.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'xyxy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 73\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# YOLOv8 detection\u001b[39;00m\n\u001b[0;32m     72\u001b[0m results \u001b[38;5;241m=\u001b[39m model(frame)\n\u001b[1;32m---> 73\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxyxy\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# YOLOv8 bounding boxes in xyxy format\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Filter detections for humans (class 0 in COCO dataset is 'person')\u001b[39;00m\n\u001b[0;32m     76\u001b[0m humans \u001b[38;5;241m=\u001b[39m [det \u001b[38;5;28;01mfor\u001b[39;00m det \u001b[38;5;129;01min\u001b[39;00m detections \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(det[\u001b[38;5;241m5\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'xyxy'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Function to compute the Intersection over Union (IoU) between two boxes\n",
    "def compute_iou(box1, box2):\n",
    "    x1_min, y1_min, x1_max, y1_max = box1\n",
    "    x2_min, y2_min, x2_max, y2_max = box2\n",
    "\n",
    "    # Compute the coordinates of the intersection box\n",
    "    x_min = max(x1_min, x2_min)\n",
    "    y_min = max(y1_min, y2_min)\n",
    "    x_max = min(x1_max, x2_max)\n",
    "    y_max = min(y1_max, y2_max)\n",
    "\n",
    "    # Compute the area of intersection\n",
    "    inter_area = max(0, x_max - x_min) * max(0, y_max - y_min)\n",
    "\n",
    "    # Compute the area of both boxes\n",
    "    box1_area = (x1_max - x1_min) * (y1_max - y1_min)\n",
    "    box2_area = (x2_max - x2_min) * (y2_max - y2_min)\n",
    "\n",
    "    # Compute the Intersection over Union (IoU)\n",
    "    iou = inter_area / (box1_area + box2_area - inter_area)\n",
    "    return iou\n",
    "\n",
    "# Function to assign track IDs to new detections based on previous detections\n",
    "def assign_track_ids(detections, previous_detections, threshold=0.5):\n",
    "    new_tracks = []\n",
    "    for detection in detections:\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        best_iou = 0\n",
    "        best_track_id = None\n",
    "\n",
    "        # Check for previous detection with the highest IoU\n",
    "        for prev_det in previous_detections:\n",
    "            prev_box = prev_det[:4]\n",
    "            prev_track_id = prev_det[4]\n",
    "            iou = compute_iou([x1, y1, x2, y2], prev_box)\n",
    "\n",
    "            if iou > best_iou and iou > threshold:\n",
    "                best_iou = iou\n",
    "                best_track_id = prev_track_id\n",
    "\n",
    "        if best_track_id is not None:\n",
    "            new_tracks.append([x1, y1, x2, y2, best_track_id])\n",
    "        else:\n",
    "            # If no match, assign a new track ID\n",
    "            new_track_id = len(previous_detections) + 1\n",
    "            new_tracks.append([x1, y1, x2, y2, new_track_id])\n",
    "\n",
    "    return new_tracks\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # YOLOv8 model (nano version for speed)\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# To store previous detections for tracking\n",
    "previous_detections = []\n",
    "\n",
    "# To store the unique track IDs for counting\n",
    "unique_ids = set()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # YOLOv8 detection\n",
    "    results = model(frame)\n",
    "    detections = results.xyxy[0].cpu().numpy()  # YOLOv8 bounding boxes in xyxy format\n",
    "\n",
    "    # Filter detections for humans (class 0 in COCO dataset is 'person')\n",
    "    humans = [det for det in detections if int(det[5]) == 0]\n",
    "\n",
    "    # Assign track IDs based on previous frame detections\n",
    "    tracked_humans = assign_track_ids(humans, previous_detections)\n",
    "\n",
    "    # Update previous detections for next frame\n",
    "    previous_detections = tracked_humans\n",
    "\n",
    "    # Track new IDs and update total count\n",
    "    for tracked_human in tracked_humans:\n",
    "        x1, y1, x2, y2, track_id = tracked_human\n",
    "        unique_ids.add(track_id)\n",
    "\n",
    "        # Draw bounding boxes and track IDs\n",
    "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"ID: {track_id}\", (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the total count of unique humans\n",
    "    total_count = len(unique_ids)\n",
    "    cv2.putText(frame, f\"Total Count: {total_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the frame with tracked humans and total count\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 109.1ms\n",
      "Speed: 2.0ms preprocess, 109.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Boxes' object has no attribute 'boxes'. See valid attributes below.\n\n    A class for managing and manipulating detection boxes.\n\n    This class provides functionality for handling detection boxes, including their coordinates, confidence scores,\n    class labels, and optional tracking IDs. It supports various box formats and offers methods for easy manipulation\n    and conversion between different coordinate systems.\n\n    Attributes:\n        data (torch.Tensor | numpy.ndarray): The raw tensor containing detection boxes and associated data.\n        orig_shape (Tuple[int, int]): The original image dimensions (height, width).\n        is_track (bool): Indicates whether tracking IDs are included in the box data.\n        xyxy (torch.Tensor | numpy.ndarray): Boxes in [x1, y1, x2, y2] format.\n        conf (torch.Tensor | numpy.ndarray): Confidence scores for each box.\n        cls (torch.Tensor | numpy.ndarray): Class labels for each box.\n        id (torch.Tensor | numpy.ndarray): Tracking IDs for each box (if available).\n        xywh (torch.Tensor | numpy.ndarray): Boxes in [x, y, width, height] format.\n        xyxyn (torch.Tensor | numpy.ndarray): Normalized [x1, y1, x2, y2] boxes relative to orig_shape.\n        xywhn (torch.Tensor | numpy.ndarray): Normalized [x, y, width, height] boxes relative to orig_shape.\n\n    Methods:\n        cpu(): Returns a copy of the object with all tensors on CPU memory.\n        numpy(): Returns a copy of the object with all tensors as numpy arrays.\n        cuda(): Returns a copy of the object with all tensors on GPU memory.\n        to(*args, **kwargs): Returns a copy of the object with tensors on specified device and dtype.\n\n    Examples:\n        >>> import torch\n        >>> boxes_data = torch.tensor([[100, 50, 150, 100, 0.9, 0], [200, 150, 300, 250, 0.8, 1]])\n        >>> orig_shape = (480, 640)  # height, width\n        >>> boxes = Boxes(boxes_data, orig_shape)\n        >>> print(boxes.xyxy)\n        >>> print(boxes.conf)\n        >>> print(boxes.cls)\n        >>> print(boxes.xywhn)\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m results \u001b[38;5;241m=\u001b[39m model(frame)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Accessing the list of detections directly (this is a list of dictionaries)\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboxes\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# YOLOv8 bounding boxes in xyxy format\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Filter detections for humans (class 0 in COCO dataset is 'person')\u001b[39;00m\n\u001b[0;32m     78\u001b[0m humans \u001b[38;5;241m=\u001b[39m [det \u001b[38;5;28;01mfor\u001b[39;00m det \u001b[38;5;129;01min\u001b[39;00m detections \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(det[\u001b[38;5;241m5\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\harih\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\utils\\__init__.py:219\u001b[0m, in \u001b[0;36mSimpleClass.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Custom attribute access error message with helpful information.\"\"\"\u001b[39;00m\n\u001b[0;32m    218\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m--> 219\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. See valid attributes below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Boxes' object has no attribute 'boxes'. See valid attributes below.\n\n    A class for managing and manipulating detection boxes.\n\n    This class provides functionality for handling detection boxes, including their coordinates, confidence scores,\n    class labels, and optional tracking IDs. It supports various box formats and offers methods for easy manipulation\n    and conversion between different coordinate systems.\n\n    Attributes:\n        data (torch.Tensor | numpy.ndarray): The raw tensor containing detection boxes and associated data.\n        orig_shape (Tuple[int, int]): The original image dimensions (height, width).\n        is_track (bool): Indicates whether tracking IDs are included in the box data.\n        xyxy (torch.Tensor | numpy.ndarray): Boxes in [x1, y1, x2, y2] format.\n        conf (torch.Tensor | numpy.ndarray): Confidence scores for each box.\n        cls (torch.Tensor | numpy.ndarray): Class labels for each box.\n        id (torch.Tensor | numpy.ndarray): Tracking IDs for each box (if available).\n        xywh (torch.Tensor | numpy.ndarray): Boxes in [x, y, width, height] format.\n        xyxyn (torch.Tensor | numpy.ndarray): Normalized [x1, y1, x2, y2] boxes relative to orig_shape.\n        xywhn (torch.Tensor | numpy.ndarray): Normalized [x, y, width, height] boxes relative to orig_shape.\n\n    Methods:\n        cpu(): Returns a copy of the object with all tensors on CPU memory.\n        numpy(): Returns a copy of the object with all tensors as numpy arrays.\n        cuda(): Returns a copy of the object with all tensors on GPU memory.\n        to(*args, **kwargs): Returns a copy of the object with tensors on specified device and dtype.\n\n    Examples:\n        >>> import torch\n        >>> boxes_data = torch.tensor([[100, 50, 150, 100, 0.9, 0], [200, 150, 300, 250, 0.8, 1]])\n        >>> orig_shape = (480, 640)  # height, width\n        >>> boxes = Boxes(boxes_data, orig_shape)\n        >>> print(boxes.xyxy)\n        >>> print(boxes.conf)\n        >>> print(boxes.cls)\n        >>> print(boxes.xywhn)\n    "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Function to compute the Intersection over Union (IoU) between two boxes\n",
    "def compute_iou(box1, box2):\n",
    "    x1_min, y1_min, x1_max, y1_max = box1\n",
    "    x2_min, y2_min, x2_max, y2_max = box2\n",
    "\n",
    "    # Compute the coordinates of the intersection box\n",
    "    x_min = max(x1_min, x2_min)\n",
    "    y_min = max(y1_min, y2_min)\n",
    "    x_max = min(x1_max, x2_max)\n",
    "    y_max = min(y1_max, y2_max)\n",
    "\n",
    "    # Compute the area of intersection\n",
    "    inter_area = max(0, x_max - x_min) * max(0, y_max - y_min)\n",
    "\n",
    "    # Compute the area of both boxes\n",
    "    box1_area = (x1_max - x1_min) * (y1_max - y1_min)\n",
    "    box2_area = (x2_max - x2_min) * (y2_max - y2_min)\n",
    "\n",
    "    # Compute the Intersection over Union (IoU)\n",
    "    iou = inter_area / (box1_area + box2_area - inter_area)\n",
    "    return iou\n",
    "\n",
    "# Function to assign track IDs to new detections based on previous detections\n",
    "def assign_track_ids(detections, previous_detections, threshold=0.5):\n",
    "    new_tracks = []\n",
    "    for detection in detections:\n",
    "        x1, y1, x2, y2, conf, cls = detection\n",
    "        best_iou = 0\n",
    "        best_track_id = None\n",
    "\n",
    "        # Check for previous detection with the highest IoU\n",
    "        for prev_det in previous_detections:\n",
    "            prev_box = prev_det[:4]\n",
    "            prev_track_id = prev_det[4]\n",
    "            iou = compute_iou([x1, y1, x2, y2], prev_box)\n",
    "\n",
    "            if iou > best_iou and iou > threshold:\n",
    "                best_iou = iou\n",
    "                best_track_id = prev_track_id\n",
    "\n",
    "        if best_track_id is not None:\n",
    "            new_tracks.append([x1, y1, x2, y2, best_track_id])\n",
    "        else:\n",
    "            # If no match, assign a new track ID\n",
    "            new_track_id = len(previous_detections) + 1\n",
    "            new_tracks.append([x1, y1, x2, y2, new_track_id])\n",
    "\n",
    "    return new_tracks\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # YOLOv8 model (nano version for speed)\n",
    "\n",
    "# Initialize webcam capture (use 0 for the default webcam, or change it for another camera)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# To store previous detections for tracking\n",
    "previous_detections = []\n",
    "\n",
    "# To store the unique track IDs for counting\n",
    "unique_ids = set()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # YOLOv8 detection\n",
    "    results = model(frame)\n",
    "\n",
    "    # Accessing the list of detections directly (this is a list of dictionaries)\n",
    "    detections = results[0].boxes.boxes.cpu().numpy()  # YOLOv8 bounding boxes in xyxy format\n",
    "\n",
    "    # Filter detections for humans (class 0 in COCO dataset is 'person')\n",
    "    humans = [det for det in detections if int(det[5]) == 0]\n",
    "\n",
    "    # Assign track IDs based on previous frame detections\n",
    "    tracked_humans = assign_track_ids(humans, previous_detections)\n",
    "\n",
    "    # Update previous detections for next frame\n",
    "    previous_detections = tracked_humans\n",
    "\n",
    "    # Track new IDs and update total count\n",
    "    for tracked_human in tracked_humans:\n",
    "        x1, y1, x2, y2, track_id = tracked_human\n",
    "        unique_ids.add(track_id)\n",
    "\n",
    "        # Draw bounding boxes and track IDs\n",
    "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"ID: {track_id}\", (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the total count of unique humans\n",
    "    total_count = len(unique_ids)\n",
    "    cv2.putText(frame, f\"Total Count: {total_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the frame with tracked humans and total count\n",
    "    cv2.imshow('Webcam', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 179.8ms\n",
      "Speed: 4.0ms preprocess, 179.8ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[428, 19]\n",
      "[428, 19]\n",
      "\n",
      "0: 384x640 1 person, 149.2ms\n",
      "Speed: 2.9ms preprocess, 149.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[428, 19]\n",
      "\n",
      "0: 384x640 1 person, 157.2ms\n",
      "Speed: 2.0ms preprocess, 157.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 147.0ms\n",
      "Speed: 2.4ms preprocess, 147.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 148.0ms\n",
      "Speed: 2.1ms preprocess, 148.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 145.0ms\n",
      "Speed: 3.0ms preprocess, 145.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 141.4ms\n",
      "Speed: 2.0ms preprocess, 141.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 143.4ms\n",
      "Speed: 2.4ms preprocess, 143.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 141.0ms\n",
      "Speed: 2.0ms preprocess, 141.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 145.0ms\n",
      "Speed: 2.5ms preprocess, 145.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 149.6ms\n",
      "Speed: 2.2ms preprocess, 149.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.0ms\n",
      "Speed: 2.0ms preprocess, 138.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 145.0ms\n",
      "Speed: 2.5ms preprocess, 145.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.5ms\n",
      "Speed: 2.2ms preprocess, 139.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.5ms\n",
      "Speed: 2.5ms preprocess, 140.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 174.2ms\n",
      "Speed: 2.4ms preprocess, 174.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 134.0ms\n",
      "Speed: 2.1ms preprocess, 134.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.2ms\n",
      "Speed: 2.2ms preprocess, 138.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 142.4ms\n",
      "Speed: 2.3ms preprocess, 142.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.5ms\n",
      "Speed: 1.0ms preprocess, 140.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.9ms\n",
      "Speed: 2.5ms preprocess, 139.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 137.2ms\n",
      "Speed: 1.0ms preprocess, 137.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.6ms\n",
      "Speed: 2.3ms preprocess, 138.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 153.0ms\n",
      "Speed: 2.0ms preprocess, 153.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 180.7ms\n",
      "Speed: 3.0ms preprocess, 180.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 183.1ms\n",
      "Speed: 2.2ms preprocess, 183.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 181.6ms\n",
      "Speed: 2.9ms preprocess, 181.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 184.5ms\n",
      "Speed: 1.6ms preprocess, 184.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 188.9ms\n",
      "Speed: 2.5ms preprocess, 188.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 193.3ms\n",
      "Speed: 2.2ms preprocess, 193.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 185.4ms\n",
      "Speed: 2.1ms preprocess, 185.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 182.9ms\n",
      "Speed: 2.8ms preprocess, 182.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 177.8ms\n",
      "Speed: 2.3ms preprocess, 177.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 188.5ms\n",
      "Speed: 1.2ms preprocess, 188.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 186.5ms\n",
      "Speed: 2.0ms preprocess, 186.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[291, 117]\n",
      "\n",
      "0: 384x640 1 person, 159.3ms\n",
      "Speed: 1.7ms preprocess, 159.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[288, 124]\n",
      "\n",
      "0: 384x640 1 person, 180.7ms\n",
      "Speed: 2.6ms preprocess, 180.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 179.0ms\n",
      "Speed: 2.6ms preprocess, 179.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 179.8ms\n",
      "Speed: 2.5ms preprocess, 179.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 166.6ms\n",
      "Speed: 2.3ms preprocess, 166.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 149.1ms\n",
      "Speed: 2.1ms preprocess, 149.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 146.5ms\n",
      "Speed: 2.5ms preprocess, 146.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 144.9ms\n",
      "Speed: 1.5ms preprocess, 144.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 137.8ms\n",
      "Speed: 1.4ms preprocess, 137.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.4ms\n",
      "Speed: 2.1ms preprocess, 139.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 137.8ms\n",
      "Speed: 2.7ms preprocess, 137.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 142.2ms\n",
      "Speed: 2.4ms preprocess, 142.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.1ms\n",
      "Speed: 2.4ms preprocess, 138.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.8ms\n",
      "Speed: 2.0ms preprocess, 139.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.6ms\n",
      "Speed: 2.5ms preprocess, 139.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.9ms\n",
      "Speed: 2.0ms preprocess, 139.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.9ms\n",
      "Speed: 2.0ms preprocess, 139.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.3ms\n",
      "Speed: 1.3ms preprocess, 140.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.6ms\n",
      "Speed: 2.0ms preprocess, 138.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.1ms\n",
      "Speed: 1.4ms preprocess, 138.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 134.5ms\n",
      "Speed: 1.4ms preprocess, 134.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 136.9ms\n",
      "Speed: 1.9ms preprocess, 136.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 152.7ms\n",
      "Speed: 3.0ms preprocess, 152.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.1ms\n",
      "Speed: 2.0ms preprocess, 138.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 152.4ms\n",
      "Speed: 2.6ms preprocess, 152.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.6ms\n",
      "Speed: 2.5ms preprocess, 138.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.6ms\n",
      "Speed: 2.9ms preprocess, 138.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.7ms\n",
      "Speed: 1.3ms preprocess, 140.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 137.1ms\n",
      "Speed: 2.0ms preprocess, 137.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 135.2ms\n",
      "Speed: 1.2ms preprocess, 135.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 137.1ms\n",
      "Speed: 1.5ms preprocess, 137.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.2ms\n",
      "Speed: 1.9ms preprocess, 138.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.0ms\n",
      "Speed: 1.2ms preprocess, 139.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 141.2ms\n",
      "Speed: 1.0ms preprocess, 141.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.2ms\n",
      "Speed: 2.0ms preprocess, 138.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.3ms\n",
      "Speed: 1.5ms preprocess, 140.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 141.4ms\n",
      "Speed: 2.1ms preprocess, 141.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.2ms\n",
      "Speed: 2.0ms preprocess, 140.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.6ms\n",
      "Speed: 2.4ms preprocess, 139.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.3ms\n",
      "Speed: 1.3ms preprocess, 140.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.9ms\n",
      "Speed: 1.2ms preprocess, 140.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.6ms\n",
      "Speed: 2.4ms preprocess, 138.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 141.2ms\n",
      "Speed: 2.5ms preprocess, 141.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 162.0ms\n",
      "Speed: 2.0ms preprocess, 162.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 174.4ms\n",
      "Speed: 2.6ms preprocess, 174.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 155.7ms\n",
      "Speed: 2.0ms preprocess, 155.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 135.8ms\n",
      "Speed: 2.5ms preprocess, 135.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.3ms\n",
      "Speed: 1.4ms preprocess, 139.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 168.9ms\n",
      "Speed: 2.1ms preprocess, 168.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 149.8ms\n",
      "Speed: 2.1ms preprocess, 149.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 169.0ms\n",
      "Speed: 3.2ms preprocess, 169.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 169.0ms\n",
      "Speed: 1.4ms preprocess, 169.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 149.7ms\n",
      "Speed: 2.2ms preprocess, 149.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 145.3ms\n",
      "Speed: 1.6ms preprocess, 145.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 141.4ms\n",
      "Speed: 1.4ms preprocess, 141.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 141.0ms\n",
      "Speed: 2.1ms preprocess, 141.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 144.0ms\n",
      "Speed: 2.2ms preprocess, 144.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 145.2ms\n",
      "Speed: 2.3ms preprocess, 145.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.4ms\n",
      "Speed: 2.0ms preprocess, 138.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 144.4ms\n",
      "Speed: 1.3ms preprocess, 144.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.7ms\n",
      "Speed: 2.0ms preprocess, 139.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 156.2ms\n",
      "Speed: 2.0ms preprocess, 156.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 160.8ms\n",
      "Speed: 1.2ms preprocess, 160.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 143.7ms\n",
      "Speed: 2.0ms preprocess, 143.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 137.7ms\n",
      "Speed: 2.3ms preprocess, 137.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 137.6ms\n",
      "Speed: 1.0ms preprocess, 137.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.8ms\n",
      "Speed: 2.0ms preprocess, 138.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 137.1ms\n",
      "Speed: 1.3ms preprocess, 137.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 162.4ms\n",
      "Speed: 2.2ms preprocess, 162.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.7ms\n",
      "Speed: 1.0ms preprocess, 139.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.3ms\n",
      "Speed: 2.6ms preprocess, 139.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 141.1ms\n",
      "Speed: 2.5ms preprocess, 141.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 137.7ms\n",
      "Speed: 1.9ms preprocess, 137.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.1ms\n",
      "Speed: 2.2ms preprocess, 138.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 137.9ms\n",
      "Speed: 1.2ms preprocess, 137.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.2ms\n",
      "Speed: 1.4ms preprocess, 139.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.8ms\n",
      "Speed: 2.2ms preprocess, 138.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.8ms\n",
      "Speed: 2.4ms preprocess, 138.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 137.9ms\n",
      "Speed: 1.4ms preprocess, 137.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 137.2ms\n",
      "Speed: 2.2ms preprocess, 137.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.1ms\n",
      "Speed: 1.0ms preprocess, 140.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 147.2ms\n",
      "Speed: 3.2ms preprocess, 147.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 133.8ms\n",
      "Speed: 2.5ms preprocess, 133.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.0ms\n",
      "Speed: 2.1ms preprocess, 138.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 137.7ms\n",
      "Speed: 2.0ms preprocess, 137.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.9ms\n",
      "Speed: 1.5ms preprocess, 138.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 141.1ms\n",
      "Speed: 2.1ms preprocess, 141.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 136.8ms\n",
      "Speed: 2.0ms preprocess, 136.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 137.2ms\n",
      "Speed: 2.0ms preprocess, 137.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 163.6ms\n",
      "Speed: 1.4ms preprocess, 163.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 135.6ms\n",
      "Speed: 2.6ms preprocess, 135.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.7ms\n",
      "Speed: 2.3ms preprocess, 138.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.5ms\n",
      "Speed: 1.9ms preprocess, 138.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 135.2ms\n",
      "Speed: 2.3ms preprocess, 135.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 136.4ms\n",
      "Speed: 1.0ms preprocess, 136.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.0ms\n",
      "Speed: 2.0ms preprocess, 138.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.2ms\n",
      "Speed: 2.5ms preprocess, 139.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 135.3ms\n",
      "Speed: 2.4ms preprocess, 135.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 136.7ms\n",
      "Speed: 2.2ms preprocess, 136.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.2ms\n",
      "Speed: 2.5ms preprocess, 139.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 137.5ms\n",
      "Speed: 2.5ms preprocess, 137.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 136.8ms\n",
      "Speed: 2.2ms preprocess, 136.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 136.3ms\n",
      "Speed: 2.5ms preprocess, 136.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 136.9ms\n",
      "Speed: 1.7ms preprocess, 136.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.1ms\n",
      "Speed: 1.0ms preprocess, 140.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.1ms\n",
      "Speed: 2.5ms preprocess, 138.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 142.2ms\n",
      "Speed: 1.3ms preprocess, 142.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.5ms\n",
      "Speed: 2.4ms preprocess, 139.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 137.5ms\n",
      "Speed: 1.0ms preprocess, 137.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 174.5ms\n",
      "Speed: 2.5ms preprocess, 174.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 142.5ms\n",
      "Speed: 2.0ms preprocess, 142.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 161.5ms\n",
      "Speed: 2.5ms preprocess, 161.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 142.1ms\n",
      "Speed: 1.2ms preprocess, 142.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 137.9ms\n",
      "Speed: 2.3ms preprocess, 137.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 137.3ms\n",
      "Speed: 2.6ms preprocess, 137.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 146.2ms\n",
      "Speed: 1.0ms preprocess, 146.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 174.5ms\n",
      "Speed: 1.2ms preprocess, 174.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 186.6ms\n",
      "Speed: 2.3ms preprocess, 186.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 136.9ms\n",
      "Speed: 2.0ms preprocess, 136.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 137.0ms\n",
      "Speed: 2.1ms preprocess, 137.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 182.2ms\n",
      "Speed: 2.3ms preprocess, 182.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 194.5ms\n",
      "Speed: 3.0ms preprocess, 194.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 140.0ms\n",
      "Speed: 1.5ms preprocess, 140.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 137.5ms\n",
      "Speed: 2.4ms preprocess, 137.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 140.4ms\n",
      "Speed: 1.3ms preprocess, 140.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 142.1ms\n",
      "Speed: 1.3ms preprocess, 142.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 137.5ms\n",
      "Speed: 2.5ms preprocess, 137.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.3ms\n",
      "Speed: 2.4ms preprocess, 138.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 138.7ms\n",
      "Speed: 1.5ms preprocess, 138.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 136.1ms\n",
      "Speed: 2.6ms preprocess, 136.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 140.3ms\n",
      "Speed: 2.2ms preprocess, 140.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 139.0ms\n",
      "Speed: 2.0ms preprocess, 139.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 143.3ms\n",
      "Speed: 2.3ms preprocess, 143.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 145.3ms\n",
      "Speed: 2.3ms preprocess, 145.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 143.2ms\n",
      "Speed: 1.0ms preprocess, 143.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 141.1ms\n",
      "Speed: 2.0ms preprocess, 141.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.9ms\n",
      "Speed: 2.1ms preprocess, 138.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 141.4ms\n",
      "Speed: 1.5ms preprocess, 141.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.3ms\n",
      "Speed: 1.5ms preprocess, 139.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.6ms\n",
      "Speed: 2.4ms preprocess, 139.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 141.0ms\n",
      "Speed: 1.5ms preprocess, 141.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 141.0ms\n",
      "Speed: 2.1ms preprocess, 141.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.6ms\n",
      "Speed: 1.2ms preprocess, 139.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.1ms\n",
      "Speed: 2.3ms preprocess, 138.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.5ms\n",
      "Speed: 2.0ms preprocess, 140.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.9ms\n",
      "Speed: 2.6ms preprocess, 140.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 135.5ms\n",
      "Speed: 2.4ms preprocess, 135.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 143.8ms\n",
      "Speed: 2.2ms preprocess, 143.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 142.5ms\n",
      "Speed: 2.0ms preprocess, 142.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.1ms\n",
      "Speed: 1.3ms preprocess, 138.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.9ms\n",
      "Speed: 2.1ms preprocess, 138.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 141.6ms\n",
      "Speed: 1.5ms preprocess, 141.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 144.1ms\n",
      "Speed: 1.0ms preprocess, 144.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 142.4ms\n",
      "Speed: 2.0ms preprocess, 142.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 154.0ms\n",
      "Speed: 1.5ms preprocess, 154.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 160.1ms\n",
      "Speed: 2.0ms preprocess, 160.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.7ms\n",
      "Speed: 2.4ms preprocess, 139.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.1ms\n",
      "Speed: 2.0ms preprocess, 139.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 136.9ms\n",
      "Speed: 2.5ms preprocess, 136.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 144.1ms\n",
      "Speed: 1.2ms preprocess, 144.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.0ms\n",
      "Speed: 2.7ms preprocess, 140.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.4ms\n",
      "Speed: 1.5ms preprocess, 138.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.2ms\n",
      "Speed: 2.4ms preprocess, 139.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.5ms\n",
      "Speed: 1.5ms preprocess, 140.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 151.2ms\n",
      "Speed: 2.4ms preprocess, 151.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 134.5ms\n",
      "Speed: 2.0ms preprocess, 134.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.1ms\n",
      "Speed: 2.0ms preprocess, 138.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.5ms\n",
      "Speed: 2.3ms preprocess, 139.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.0ms\n",
      "Speed: 2.0ms preprocess, 139.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.1ms\n",
      "Speed: 2.4ms preprocess, 138.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.1ms\n",
      "Speed: 2.5ms preprocess, 140.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.9ms\n",
      "Speed: 1.0ms preprocess, 139.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 141.9ms\n",
      "Speed: 2.9ms preprocess, 141.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.6ms\n",
      "Speed: 1.8ms preprocess, 140.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.6ms\n",
      "Speed: 1.4ms preprocess, 139.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.7ms\n",
      "Speed: 2.1ms preprocess, 138.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.4ms\n",
      "Speed: 1.0ms preprocess, 139.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 160.8ms\n",
      "Speed: 2.8ms preprocess, 160.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 139.9ms\n",
      "Speed: 2.4ms preprocess, 139.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 140.7ms\n",
      "Speed: 2.5ms preprocess, 140.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 141.6ms\n",
      "Speed: 2.0ms preprocess, 141.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 136.2ms\n",
      "Speed: 2.7ms preprocess, 136.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 150.5ms\n",
      "Speed: 2.0ms preprocess, 150.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 147.8ms\n",
      "Speed: 2.0ms preprocess, 147.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 138.2ms\n",
      "Speed: 2.5ms preprocess, 138.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 145.3ms\n",
      "Speed: 2.1ms preprocess, 145.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 144.2ms\n",
      "Speed: 2.4ms preprocess, 144.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.7ms\n",
      "Speed: 2.1ms preprocess, 140.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 142.8ms\n",
      "Speed: 2.4ms preprocess, 142.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 141.6ms\n",
      "Speed: 2.0ms preprocess, 141.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 142.6ms\n",
      "Speed: 2.4ms preprocess, 142.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.2ms\n",
      "Speed: 2.4ms preprocess, 140.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 143.2ms\n",
      "Speed: 2.1ms preprocess, 143.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 143.7ms\n",
      "Speed: 2.6ms preprocess, 143.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 144.0ms\n",
      "Speed: 2.4ms preprocess, 144.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 144.4ms\n",
      "Speed: 2.3ms preprocess, 144.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 141.8ms\n",
      "Speed: 1.6ms preprocess, 141.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 145.7ms\n",
      "Speed: 2.2ms preprocess, 145.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 145.7ms\n",
      "Speed: 1.5ms preprocess, 145.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.3ms\n",
      "Speed: 2.4ms preprocess, 138.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 189.6ms\n",
      "Speed: 2.5ms preprocess, 189.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 138.9ms\n",
      "Speed: 2.3ms preprocess, 138.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 144.5ms\n",
      "Speed: 2.3ms preprocess, 144.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 142.0ms\n",
      "Speed: 2.5ms preprocess, 142.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.1ms\n",
      "Speed: 1.3ms preprocess, 139.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.6ms\n",
      "Speed: 2.0ms preprocess, 139.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 142.3ms\n",
      "Speed: 2.6ms preprocess, 142.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 143.2ms\n",
      "Speed: 2.0ms preprocess, 143.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 136.7ms\n",
      "Speed: 2.5ms preprocess, 136.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.6ms\n",
      "Speed: 2.3ms preprocess, 139.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 145.0ms\n",
      "Speed: 2.3ms preprocess, 145.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 142.0ms\n",
      "Speed: 2.4ms preprocess, 142.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.9ms\n",
      "Speed: 2.7ms preprocess, 139.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.4ms\n",
      "Speed: 2.0ms preprocess, 139.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.2ms\n",
      "Speed: 2.3ms preprocess, 140.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.4ms\n",
      "Speed: 2.2ms preprocess, 139.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 142.2ms\n",
      "Speed: 2.0ms preprocess, 142.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 144.0ms\n",
      "Speed: 1.2ms preprocess, 144.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.4ms\n",
      "Speed: 2.1ms preprocess, 140.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 146.7ms\n",
      "Speed: 2.0ms preprocess, 146.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.5ms\n",
      "Speed: 2.5ms preprocess, 140.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 141.3ms\n",
      "Speed: 2.0ms preprocess, 141.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 142.8ms\n",
      "Speed: 2.0ms preprocess, 142.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 152.9ms\n",
      "Speed: 2.8ms preprocess, 152.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 142.4ms\n",
      "Speed: 1.9ms preprocess, 142.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 142.2ms\n",
      "Speed: 2.2ms preprocess, 142.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.7ms\n",
      "Speed: 2.5ms preprocess, 139.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 141.4ms\n",
      "Speed: 1.4ms preprocess, 141.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.8ms\n",
      "Speed: 2.3ms preprocess, 140.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 140.6ms\n",
      "Speed: 1.5ms preprocess, 140.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 142.8ms\n",
      "Speed: 1.6ms preprocess, 142.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 146.9ms\n",
      "Speed: 2.6ms preprocess, 146.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 151.2ms\n",
      "Speed: 2.0ms preprocess, 151.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.0ms\n",
      "Speed: 2.3ms preprocess, 139.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 144.3ms\n",
      "Speed: 3.2ms preprocess, 144.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 139.1ms\n",
      "Speed: 2.0ms preprocess, 139.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 142.3ms\n",
      "Speed: 2.0ms preprocess, 142.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import cvzone\n",
    "import numpy as np\n",
    "\n",
    "def RGB(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_MOUSEMOVE:\n",
    "        point = [x, y]\n",
    "        print(point)\n",
    "\n",
    "cv2.namedWindow('RGB')\n",
    "cv2.setMouseCallback('RGB', RGB)\n",
    "\n",
    "# Load the YOLO11 model\n",
    "model = YOLO(\"yolo11s.pt\")\n",
    "names=model.model.names\n",
    "# Open the video file (use video file or webcam, here using webcam)\n",
    "cap = cv2.VideoCapture(0)\n",
    "count=0\n",
    "cy1=261\n",
    "cy2=286\n",
    "offset=8\n",
    "inp={}\n",
    "enter=[]\n",
    "exp={}\n",
    "exitp=[]\n",
    "while True:\n",
    "    ret,frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    count += 1\n",
    "    if count % 3 != 0:\n",
    "        continue\n",
    "\n",
    "    frame = cv2.resize(frame, (1020, 600))\n",
    "    \n",
    "    # Run YOLO11 tracking on the frame, persisting tracks between frames\n",
    "    results = model.track(frame, persist=True,classes=0)\n",
    "\n",
    "    # Check if there are any boxes in the results\n",
    "    if results[0].boxes is not None and results[0].boxes.id is not None:\n",
    "        # Get the boxes (x, y, w, h), class IDs, track IDs, and confidences\n",
    "        boxes = results[0].boxes.xyxy.int().cpu().tolist()  # Bounding boxes\n",
    "        class_ids = results[0].boxes.cls.int().cpu().tolist()  # Class IDs\n",
    "        track_ids = results[0].boxes.id.int().cpu().tolist()  # Track IDs\n",
    "        confidences = results[0].boxes.conf.cpu().tolist()  # Confidence score\n",
    "       \n",
    "        for box, class_id, track_id, conf in zip(boxes, class_ids, track_ids, confidences):\n",
    "            c = names[class_id]\n",
    "            x1, y1, x2, y2 = box\n",
    "            cv2.rectangle(frame,(x1,y1),(x2,y2),(0,255,0),2)\n",
    "            cvzone.putTextRect(frame,f'{track_id}',(x1,y2),1,1)\n",
    "            cvzone.putTextRect(frame,f'{c}',(x1,y1),1,1)\n",
    "                  \n",
    "\n",
    "#    cv2.line(frame,(440,286),(1018,286),(0,0,255),2)\n",
    "#    cv2.line(frame,(438,261),(1018,261),(255,0,255),2)\n",
    "\n",
    "    cv2.imshow(\"RGB\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "       break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
